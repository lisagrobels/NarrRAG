# NTLRAG - Narrative Topic Labels derived with Retrieval Augmented Generation

NTLRAG is a modular narrative labeling framework that generates semantically precise and human-interpretable topic labels while minimizing human effort. NTLRAG uses standard topic model outputs to generate, validate, and refine narratives that serve as topic labels. The orchestrated RAG framework uses multiple retrieval strategies and chain-of-thought elements to provide high-quality output. It extracts narratives from document clusters using the following structure:

1. one or more actor(s),
2. an action associated with the actor(s), and
3. an event linking these components.
   
To improve interpretability, we additionally include a concise descriptive text that connects these three elements. This four component structure is further used as a schema for topic labels extracted and generated by NTLRAG.

This repository serves as a demo implementation of NTLRAG using LangChain, LangGraph, Ollama (llama3.2) and Pydantic. We use a subsample (1,000 posts) of this X dataset on the U.S. election in 2024 (https://github.com/sinking8/x-24-us-election, see https://arxiv.org/abs/2411.00376 for the corresponding publication) to showcase basic functionality.

Three input documents are needed for NTLRAG:

1. one csv file with the text of each post ('Document') and the assigned topic ('Topic'),
2. one json file with representative topic keywords, and
3. one json file with news data

To obtain news data for our demo, we used the GNews package for Python (https://github.com/ranahaani/GNews).

The Jupyter Notebook file NTLRAG_demo.ipynb in this repository provides a full implementation of NTLRAG with the sample dataset and is already linked to the necessary input files listed below. If needed, the technical components (retrievers, LLMs, datasets, etc.) can easily be adjusted. If the standard configurations (see publication for details) are to be used, the code can be run without any alternations.

NOTE: The setup is configured for usage in google colab, please adjust if you run it on your local machine.

For replication, we provide four files:
1. testdata_cleaned.csv, which is the input file for the BERTopic (https://github.com/MaartenGr/BERTopic/tree/master, https://arxiv.org/abs/2203.05794) model we need to run before NTLRAG can be applied (we tuned hyperparameter with OPTUNA, https://github.com/optuna/optuna) and is based on the U.S. election dataset (https://github.com/sinking8/x-24-us-election, see https://arxiv.org/abs/2411.00376 for the corresponding publication),
2. testdata_seedtopics.csv, which equals the output of BERTopic modeling and includes the two columns necessary for NTLRAG 'Document' and 'Topic',
3. testdata_topic_keywords.json, which includes representative keywords for each topic (as provided by BERTopic), and
4. testdata_news.json, which consists of news content used for validation and obtained via GNews package.

The figure below presents the LangGraph visualization of NTLRAG's narrative extraction and validation pipeline (grade equals the validation step):
<p align="center">
  <img src="/graph.png" width="200" alt="RAG Graph Visualization">
</p>

To learn more about NTLRAG and its functionality consult the corresponding publication: LINK WILL BE ADDED

